{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers seqeval[gpu]\n",
        "!pip install duckling\n",
        "!pip install --force-reinstall JPype1==0.6.3\n",
        "!pip install datetime\n",
        "\n",
        "!pip3 install torch==1.10.2+cu113 torchvision==0.11.3+cu113 torchaudio==0.10.2+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "!pip3 install pandas\n",
        "!pip3 install numpy\n",
        "!pip3 install sklearn\n",
        "!pip3 install tqdm\n",
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhUkOE1l4kDw",
        "outputId": "d595d74f-5cd2-45db-8feb-2d71fc69badb"
      },
      "id": "KhUkOE1l4kDw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seqeval[gpu]\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.10/dist-packages (from seqeval[gpu]) (1.2.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.21.3->seqeval[gpu]) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16165 sha256=0f9322efd189f0be76aadc8f2ae4282528b5911da4cde737fe08292919dcc1cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "Successfully built seqeval\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers, seqeval\n",
            "Successfully installed huggingface-hub-0.15.1 safetensors-0.3.1 seqeval-1.2.2 tokenizers-0.13.3 transformers-4.30.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting duckling\n",
            "  Downloading duckling-1.8.0-py2.py3-none-any.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting JPype1 (from duckling)\n",
            "  Downloading JPype1-1.4.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (465 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.3/465.3 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from duckling) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from duckling) (1.16.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1->duckling) (23.1)\n",
            "Installing collected packages: JPype1, duckling\n",
            "Successfully installed JPype1-1.4.1 duckling-1.8.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting JPype1==0.6.3\n",
            "  Downloading JPype1-0.6.3.tar.gz (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.5/168.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: JPype1\n",
            "  Building wheel for JPype1 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for JPype1: filename=JPype1-0.6.3-cp310-cp310-linux_x86_64.whl size=2873312 sha256=da4ccf261dd1e648da9aecb612a9729d8c16ac1a44f838f692aa48fe25c5c0d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/16/16/c49fd41c31ceca2dd05ee9f1c71416b466f19b2f099bdb59fb\n",
            "Successfully built JPype1\n",
            "Installing collected packages: JPype1\n",
            "  Attempting uninstall: JPype1\n",
            "    Found existing installation: JPype1 1.4.1\n",
            "    Uninstalling JPype1-1.4.1:\n",
            "      Successfully uninstalled JPype1-1.4.1\n",
            "Successfully installed JPype1-0.6.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datetime\n",
            "  Downloading DateTime-5.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting zope.interface (from datetime)\n",
            "  Downloading zope.interface-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from datetime) (2022.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.interface->datetime) (67.7.2)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-5.1 zope.interface-6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.html\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.10.2+cu113 (from versions: 1.11.0, 1.11.0+cu113, 1.12.0, 1.12.0+cu113, 1.12.1, 1.12.1+cu113, 1.13.0, 1.13.1, 2.0.0, 2.0.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.10.2+cu113\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.22.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datetime import datetime\n",
        "from dateutil.relativedelta import relativedelta, MO\n",
        "from datetime import timedelta\n",
        "from duckling import DucklingWrapper\n",
        "parser = DucklingWrapper()\n",
        "\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from transformers import GPT2Model, GPT2Tokenizer\n",
        "from tqdm import tqdm\n",
        "from pattern.text.en import singularize"
      ],
      "metadata": {
        "id": "bm4eu5_D4lI0"
      },
      "id": "bm4eu5_D4lI0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e2e3ac7",
      "metadata": {
        "id": "2e2e3ac7"
      },
      "outputs": [],
      "source": [
        "# PARAMS\n",
        "MAX_LEN = 128\n",
        "device = 'cpu'\n",
        "ids_to_labels ={0: 'O',\n",
        " 1: 'B-DisplayColumns',\n",
        " 2: 'B-AggColumns',\n",
        " 3: 'I-DisplayColumns',\n",
        " 4: 'B-FilterColumns',\n",
        " 5: 'I-AggColumns',\n",
        " 6:'B-date-period',\n",
        " 7: 'I-FilterColumns',\n",
        " 8: 'I-date-period',\n",
        " 9: 'B-DerivedMetricsColumns'}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iy5p6hac5iIo",
        "outputId": "7465a302-d585-492a-8e34-99b044474a6e"
      },
      "id": "Iy5p6hac5iIo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86f0d32b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "86f0d32b",
        "outputId": "8edb5239-8f54-4f89-d464-f5f164d5e45f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My Drive/Colab Notebooks/Chatbot_github/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "245a_6jz6ZKH",
        "outputId": "e5db11b5-cd8d-4017-f5b5-6cc8eb3869ff"
      },
      "id": "245a_6jz6ZKH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Chatbot_github\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ca64bea",
      "metadata": {
        "id": "6ca64bea"
      },
      "source": [
        "### Entity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da95f565",
      "metadata": {
        "id": "da95f565"
      },
      "outputs": [],
      "source": [
        "# file reading\n",
        "with open(\"model/Entity_model.pkl\", \"rb\") as f:\n",
        "  model = pickle.load(f)\n",
        "with open(\"model/Entity_tokenizer.pkl\", \"rb\") as f:\n",
        "  tokenizer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9594bbec",
      "metadata": {
        "id": "9594bbec"
      },
      "outputs": [],
      "source": [
        "# Entity Mapping TREE\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Jun  6 16:06:11 2023\n",
        "\n",
        "@author: kaustubh.keshav\n",
        "\"\"\"\n",
        "\n",
        "files = [f for f in os.listdir('chatbot_pepsico/entities/') if '_en' in f]\n",
        "all_entities = []\n",
        "# keyword_mapping = {}\n",
        "for file in files:\n",
        "    with open(f\"chatbot_pepsico/entities/{file}\", \"rb\") as f:\n",
        "        mapping = json.load(f)\n",
        "        all_entities.extend(mapping)\n",
        "        # update_dict = {j['value']:file.split(\"_\")[0] for j in mapping}\n",
        "        # keyword_mapping.update(update_dict)\n",
        "\n",
        "tree_dict = {}\n",
        "for dic in all_entities:\n",
        "    tag = dic['value']\n",
        "    synonyms = dic['synonyms']\n",
        "\n",
        "    for synonym in synonyms:\n",
        " #       print(tree_dict)\n",
        "        symbols = synonym.split(\" \")\n",
        "        root = symbols[0]\n",
        "        root = singularize(root)\n",
        "        if root in tree_dict:\n",
        "            curr_level = tree_dict[root]\n",
        "        else:\n",
        "            tree_dict[root] = {}\n",
        "            curr_level = tree_dict[root]\n",
        "#        print(curr_level, root)\n",
        "\n",
        "        for symbol in symbols[1:]:\n",
        "            symbol = singularize(symbol)\n",
        "            if symbol in curr_level:\n",
        "                curr_level = curr_level[symbol]\n",
        "            else:\n",
        "                curr_level[symbol] = {}\n",
        "                # parent = curr_level\n",
        "                curr_level = curr_level[symbol]\n",
        "\n",
        "        curr_level['leaf'] = tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa118e1d",
      "metadata": {
        "id": "fa118e1d"
      },
      "outputs": [],
      "source": [
        "# making possible entity mapping\n",
        "tags_dic = {'DisplayColumns_entries_en.json':'DisplayColumns', 'AggColumns_entries_en.json':'AggColumns', 'DerivedMetricsColumns_entries_en.json':'DerivedMetricsColumns', 'FilterColumns_entries_en.json':'FilterColumns',\n",
        "            'forecast_entries_en.json':'forecast', 'function_entries_en.json':'function', 'mathematical_filter_entries_en.json':'mathematical_filter', 'n_rows_entries_en.json':'n_rows', 'y_col_entries_en.json':'y_col'}\n",
        "\n",
        "all_entities = []\n",
        "entity_mapping = pd.DataFrame()\n",
        "\n",
        "for file in tags_dic.keys():\n",
        "    with open(f\"chatbot_pepsico/entities/{file}\", \"rb\") as f:\n",
        "        mapping = json.load(f)\n",
        "        df1 = pd.DataFrame(mapping)\n",
        "        df1['entity'] = tags_dic[file]\n",
        "        entity_mapping = pd.concat([entity_mapping, df1[['value', 'entity']]], ignore_index=True)\n",
        "\n",
        "entity_mapping_dic = {}\n",
        "for val in list(entity_mapping.value.unique()):\n",
        "  entity_mapping_dic[val] = list(entity_mapping[entity_mapping.value == val]['entity'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efc7b1b1",
      "metadata": {
        "id": "efc7b1b1"
      },
      "outputs": [],
      "source": [
        "def word_correction(sentence):\n",
        "  node = tree_dict\n",
        "  # for word in sentence.split(\" \"):\n",
        "  i = 0\n",
        "  curr_i = 0\n",
        "  words = sentence.split(\" \")\n",
        "  token_entity_map = {}\n",
        "  tokens = []\n",
        "\n",
        "  while curr_i < len(words) and i < len(words):\n",
        "      word = words[i]\n",
        "      try:\n",
        "          node = node[word]\n",
        "          i += 1\n",
        "          tokens.append(word)\n",
        "      except:\n",
        "          try:\n",
        "              token_entity_map[' '.join(tokens)] = node['leaf']\n",
        "              node = tree_dict\n",
        "              curr_i = i #here is the change. earlier it was curr_i = i + 1\n",
        "              i = curr_i\n",
        "\n",
        "          except:\n",
        "              node = tree_dict\n",
        "              i = curr_i + 1\n",
        "              curr_i = i\n",
        "\n",
        "          tokens = []\n",
        "\n",
        "  return token_entity_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a62058ce",
      "metadata": {
        "id": "a62058ce"
      },
      "outputs": [],
      "source": [
        "def response(sentence):\n",
        "\n",
        "  inputs = tokenizer(sentence.split(),\n",
        "                      is_split_into_words=True,\n",
        "                      return_offsets_mapping=True,\n",
        "                      padding='max_length',\n",
        "                      truncation=True,\n",
        "                      max_length=MAX_LEN,\n",
        "                      return_tensors=\"pt\")\n",
        "\n",
        "  # move to gpu\n",
        "  ids = inputs[\"input_ids\"].to(device)\n",
        "  mask = inputs[\"attention_mask\"].to(device)\n",
        "  # forward pass\n",
        "  outputs = model(ids, attention_mask=mask)\n",
        "  logits = outputs[0]\n",
        "\n",
        "  active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "  flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n",
        "\n",
        "  tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n",
        "  token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n",
        "  wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n",
        "\n",
        "  prediction = []\n",
        "  for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n",
        "    #only predictions on first word pieces are important\n",
        "    if mapping[0] == 0 and mapping[1] != 0:\n",
        "      prediction.append(token_pred[1])\n",
        "    else:\n",
        "      continue\n",
        "\n",
        "  return \", \".join(prediction)\n",
        "#  print(sentence.split())\n",
        "#  print(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acec6fb6",
      "metadata": {
        "id": "acec6fb6"
      },
      "outputs": [],
      "source": [
        "def return_date(text, parser):\n",
        "  output = parser.parse_time(text)\n",
        "  date = []\n",
        "  for t in output:\n",
        "    val = t['value']['value']\n",
        "    date_dict = {}\n",
        "    if isinstance(val, dict):\n",
        "      date_dict['endDate'] = val['to']\n",
        "      date_dict['startDate'] = val['from']\n",
        "    if ~isinstance(val, dict) & (\"grain\" in t['value']):\n",
        "      grain = t['value']['grain']\n",
        "      date_dict['startDate'] = val\n",
        "      if grain == 'year':\n",
        "        new_date = datetime.strptime(val,'%Y-%m-%dT%H:%M:%S.%fZ') + relativedelta(years=1)\n",
        "        new_date = new_date.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "        date_dict['endDate'] = new_date\n",
        "      if grain == 'month':\n",
        "        new_date = datetime.strptime(val,'%Y-%m-%dT%H:%M:%S.%fZ') + relativedelta(months=1)\n",
        "        new_date = new_date.strftime('%Y-%m-%dT%H:%M:%S.%fZ')\n",
        "        date_dict['endDate'] = new_date\n",
        "\n",
        "    date.append(date_dict)\n",
        "  return date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ccd4ecb",
      "metadata": {
        "id": "4ccd4ecb"
      },
      "outputs": [],
      "source": [
        "# Mapping dictionary\n",
        "# layer 1\n",
        "\n",
        "keys = list(tags_dic.values()) + ['date-period']\n",
        "\n",
        "def Response_entity(sent):\n",
        "  map_dictionary = {}\n",
        "  sent0 = sent\n",
        "  sent = sent.replace('?', '').strip()\n",
        "\n",
        "  values = []\n",
        "  word = sent.split(' ')\n",
        "  tags = response(sent).split(', ')\n",
        "\n",
        "  sent = (' ').join([singularize(w) for w in sent.lower().split()])\n",
        "  sent = sent.lower()\n",
        "  token_entity_map = word_correction(sent+' ')\n",
        "\n",
        "#########\n",
        "  correction_dic = {}\n",
        "  idx = []\n",
        "  idx_ = []\n",
        "  for key in token_entity_map.keys():\n",
        "    index = sent.replace(key, key.replace(' ','_')).split().index(key.replace(' ','_'))\n",
        "    if 'date-period' not in tags[index]:\n",
        "      idx_.append(index)\n",
        "      try:\n",
        "        l = len(entity_mapping_dic[token_entity_map[key]])\n",
        "      except:\n",
        "        l = 0\n",
        "      if l != 1:\n",
        "        correction_dic[key] = tags[index]\n",
        "      elif l == 1:\n",
        "        correction_dic[key] = 'B-' + entity_mapping_dic[token_entity_map[key]][0]\n",
        "\n",
        "      idx = idx+list(range(index,index+len(key.split())))\n",
        "\n",
        "  word_upd = [ele for id, ele in enumerate(word) if id not in idx]\n",
        "  tags_upd = [ele for id, ele in enumerate(tags) if id not in idx]\n",
        "\n",
        "  for key, val in correction_dic.items():\n",
        "    word_upd.append(token_entity_map[key])\n",
        "    tags_upd.append(val)\n",
        "#########\n",
        "\n",
        "# layer 2\n",
        "  for tag in keys:\n",
        "    word_=[]\n",
        "    tag_=[]\n",
        "    for i in range(len(word_upd)):\n",
        "      if tag in tags_upd[i]:\n",
        "          word_.append(word_upd[i])\n",
        "          tag_.append(tags_upd[i])\n",
        "\n",
        "    ar_ = []\n",
        "    j=0\n",
        "    while j < len(word_):\n",
        "      if 'B-' in tag_[j]:\n",
        "        ar_.append(word_[j])\n",
        "      elif 'I-' in tag_[j]:\n",
        "        ar_[-1]=ar_[-1]+' '+word_[j]\n",
        "      j=j+1\n",
        "\n",
        "    values.append(ar_)\n",
        "\n",
        "  map_dic = dict(zip(keys,values))\n",
        "  map_dic['date-period'] = return_date((' ').join(map_dic['date-period']), parser)\n",
        "\n",
        "#  map_dictionary[sent0] = map_dic\n",
        "\n",
        "  return map_dic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d18d8b5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d18d8b5",
        "outputId": "6f75dcb8-ff65-41be-8deb-cd697228bb47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'DisplayColumns': [],\n",
              " 'AggColumns': [],\n",
              " 'DerivedMetricsColumns': [],\n",
              " 'FilterColumns': ['Market:US'],\n",
              " 'forecast': [],\n",
              " 'function': ['contributors'],\n",
              " 'mathematical_filter': [],\n",
              " 'n_rows': ['top'],\n",
              " 'y_col': ['Gross Sales ($)'],\n",
              " 'date-period': [{'startDate': '2022-01-01T00:00:00.000Z',\n",
              "   'endDate': '2023-01-01T00:00:00.000000Z'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "Response_entity('what are top 5 contributors for sale in us in 2022')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12631b6f",
      "metadata": {
        "id": "12631b6f"
      },
      "source": [
        "### Intent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7edc1e",
      "metadata": {
        "id": "7f7edc1e"
      },
      "outputs": [],
      "source": [
        "class SimpleGPT2SequenceClassifier(nn.Module):\n",
        "    def __init__(self, hidden_size: int, num_classes:int ,max_seq_len:int, gpt_model_name:str):\n",
        "        super(SimpleGPT2SequenceClassifier,self).__init__()\n",
        "        self.gpt2model = GPT2Model.from_pretrained(gpt_model_name)\n",
        "        self.fc1 = nn.Linear(hidden_size*max_seq_len, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, input_id, mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "                input_id: encoded inputs ids of sent.\n",
        "        \"\"\"\n",
        "        gpt_out, _ = self.gpt2model(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
        "        batch_size = gpt_out.shape[0]\n",
        "        linear_output = self.fc1(gpt_out.view(batch_size,-1))\n",
        "        return linear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d344e3",
      "metadata": {
        "id": "89d344e3"
      },
      "outputs": [],
      "source": [
        "# load trained model\n",
        "with open(\"model/Intent_model.pkl\", \"rb\") as f:\n",
        "  model_intent = pickle.load(f)\n",
        "\n",
        "with open(\"model/Intent_tokenizer.pkl\", \"rb\") as f:\n",
        "  tokenizer_intent = pickle.load(f)\n",
        "tokenizer_intent.padding_side = \"left\"\n",
        "tokenizer_intent.pad_token = tokenizer_intent.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Response_intent(sent):\n",
        "  fixed_text = \" \".join(sent.lower().split())\n",
        "\n",
        "  model_input = tokenizer_intent(fixed_text, padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "  mask = model_input['attention_mask'].cpu()\n",
        "  input_id = model_input[\"input_ids\"].squeeze(1).cpu()\n",
        "\n",
        "  output = model_intent(input_id, mask)\n",
        "  prob = torch.nn.functional.softmax(output, dim=1)[0]\n",
        "\n",
        "  labels_map = {0: 'Aggregation',\n",
        "  1: 'compare_trend',\n",
        "  2: 'Default Welcome Intent',\n",
        "  3: 'derived_metrics',\n",
        "  4: 'no_comp_trend',\n",
        "  5: 'no_trend_compare',\n",
        "  6: 'simple_graphs'}\n",
        "\n",
        "  pred_label = labels_map[output.argmax(dim=1).item()]\n",
        "  return pred_label"
      ],
      "metadata": {
        "id": "JrM9Djbz7--z"
      },
      "id": "JrM9Djbz7--z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Response_intent('sales of markets across years')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kBiwKf0I8-Jv",
        "outputId": "dc6a88eb-9a06-4569-8bf2-cd8b7eb6aef2"
      },
      "id": "kBiwKf0I8-Jv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Aggregation'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Returning Intent and Entity for a query:"
      ],
      "metadata": {
        "id": "FWCUIHS_BGpP"
      },
      "id": "FWCUIHS_BGpP"
    },
    {
      "cell_type": "code",
      "source": [
        "def Model_Response(sent):\n",
        "  Model_Response = {}\n",
        "  Model_Response['queryText'] = sent\n",
        "  Model_Response['parameters'] = Response_entity(sent)\n",
        "  Model_Response['intent'] = Response_intent(sent)\n",
        "\n",
        "  return Model_Response"
      ],
      "metadata": {
        "id": "-CLxL8d99OtL"
      },
      "id": "-CLxL8d99OtL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Model_Response('sales of markets across years')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e_FDTHx9Oww",
        "outputId": "19f6780f-21e4-4e2c-b40f-dd9f1204b6ca"
      },
      "id": "6e_FDTHx9Oww",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'queryText': 'sales of markets across years',\n",
              " 'parameters': {'DisplayColumns': ['Year'],\n",
              "  'AggColumns': ['Market'],\n",
              "  'DerivedMetricsColumns': [],\n",
              "  'FilterColumns': [],\n",
              "  'forecast': [],\n",
              "  'function': [],\n",
              "  'mathematical_filter': [],\n",
              "  'n_rows': [],\n",
              "  'y_col': ['Gross Sales ($)'],\n",
              "  'date-period': []},\n",
              " 'intent': 'Aggregation'}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}